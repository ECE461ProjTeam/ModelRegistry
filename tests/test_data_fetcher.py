"""
Comprehensive tests for the data_fetcher module.

Tests cover API integration, data processing, error handling, 
and performance claims analysis with mocked external dependencies.
"""
from src.metrics.data_fetcher import (
    safe_request, extract_repo_info, extract_hf_model_id, check_availability,
    get_huggingface_model_data, get_huggingface_dataset_data, get_github_repo_data,
    analyze_code_quality, normalize_downloads, normalize_stars, compute_size_scores,
    analyze_performance_claims, fetch_comprehensive_metrics_data
)
import pytest
import sys
import os
from unittest.mock import Mock, patch, MagicMock
from typing import Dict, Any, List

# Add src to path
sys.path.insert(0, os.path.join(
    os.path.dirname(os.path.dirname(__file__)), 'src'))


class TestUtilityFunctions:
    """Test utility functions in data_fetcher."""

    def test_extract_repo_info_valid_github_url(self):
        """Test extracting owner and repo from valid GitHub URLs."""
        assert extract_repo_info(
            "https://github.com/owner/repo") == ("owner", "repo")
        assert extract_repo_info(
            "https://github.com/facebook/react/") == ("facebook", "react")

    def test_extract_repo_info_invalid_url(self):
        """Test extracting info from invalid URLs."""
        assert extract_repo_info(
            "https://gitlab.com/owner/repo") == (None, None)
        assert extract_repo_info("not-a-url") == (None, None)
        # Not enough path segments
        assert extract_repo_info("https://github.com/owner") == (None, None)

    def test_extract_hf_model_id_valid_urls(self):
        """Test extracting model IDs from HuggingFace URLs."""
        assert extract_hf_model_id(
            "https://huggingface.co/bert-base-uncased") == "bert-base-uncased"
        assert extract_hf_model_id(
            "https://huggingface.co/facebook/bart-base") == "facebook/bart-base"
        assert extract_hf_model_id(
            "https://huggingface.co/datasets/glue") == "glue"
        assert extract_hf_model_id(
            "https://huggingface.co/datasets/user/dataset") == "user/dataset"

    def test_extract_hf_model_id_invalid_urls(self):
        """Test extracting model IDs from invalid URLs."""
        assert extract_hf_model_id("https://github.com/owner/repo") is None
        assert extract_hf_model_id("not-a-url") is None

    @patch('requests.get')
    def test_safe_request_success(self, mock_get):
        """Test successful HTTP request."""
        mock_response = Mock()
        mock_response.raise_for_status.return_value = None
        mock_get.return_value = mock_response

        result = safe_request("https://example.com")
        assert result == mock_response
        mock_get.assert_called_once_with("https://example.com", timeout=10)

    @patch('requests.get')
    def test_safe_request_failure(self, mock_get):
        """Test failed HTTP request."""
        mock_get.side_effect = Exception("Network error")

        result = safe_request("https://example.com")
        assert result is None

    @patch('requests.head')
    def test_check_availability_all_available(self, mock_head):
        """Test URL availability check when all URLs are available."""
        mock_response = Mock()
        mock_response.status_code = 200
        mock_head.return_value = mock_response

        result = check_availability(
            "https://github.com/owner/repo",
            "https://huggingface.co/datasets/data",
            "https://huggingface.co/model"
        )

        assert result["has_code"] is True
        assert result["has_dataset"] is True
        assert result["has_model"] is True
        assert result["links_ok"] is True

    @patch('requests.head')
    def test_check_availability_some_unavailable(self, mock_head):
        """Test URL availability check when some URLs are unavailable."""
        def side_effect(url, **kwargs):
            if "github.com" in url:
                response = Mock()
                response.status_code = 200
                return response
            else:
                response = Mock()
                response.status_code = 404
                return response

        mock_head.side_effect = side_effect

        result = check_availability(
            "https://github.com/owner/repo",
            "https://huggingface.co/datasets/data",
            "https://huggingface.co/model"
        )

        assert result["has_code"] is True
        assert result["has_dataset"] is False
        assert result["has_model"] is False
        assert result["links_ok"] is False  # Only 1 available, need 2+


class TestNormalizationFunctions:
    """Test normalization and scoring functions."""

    def test_normalize_downloads(self):
        """Test download count normalization."""
        assert normalize_downloads(0) == 0.0
        assert normalize_downloads(-1) == 0.0
        assert normalize_downloads(1) == 0.0  # log10(1) = 0
        assert normalize_downloads(1000) == 0.5  # log10(1000)/6 = 3/6 = 0.5
        assert normalize_downloads(1000000) == 1.0  # log10(1e6)/6 = 6/6 = 1.0
        assert normalize_downloads(10000000) == 1.0  # Capped at 1.0

    def test_normalize_stars(self):
        """Test star count normalization."""
        assert normalize_stars(0) == 0.0
        assert normalize_stars(-1) == 0.0
        assert normalize_stars(1) == 0.0  # log10(1) = 0
        assert normalize_stars(100) == 0.5  # log10(100)/4 = 2/4 = 0.5
        assert normalize_stars(10000) == 1.0  # log10(1e4)/4 = 4/4 = 1.0
        assert normalize_stars(100000) == 1.0  # Capped at 1.0

    def test_compute_size_scores_zero_size(self):
        """Test size scoring with zero or negative size."""
        result = compute_size_scores(0)
        expected = {"raspberry_pi": 0.01, "jetson_nano": 0.01,
                    "desktop_pc": 0.01, "aws_server": 0.01}
        assert result == expected

        result = compute_size_scores(-100)
        assert result == expected

    def test_compute_size_scores_various_sizes(self):
        """Test size scoring with various model sizes."""
        # 50MB model
        result = compute_size_scores(50 * 1024 * 1024)
        assert result["raspberry_pi"] == 0.5  # 50MB of 100MB threshold
        assert result["jetson_nano"] > 0.9    # Well under 1GB threshold
        assert result["desktop_pc"] > 0.99    # Well under 10GB threshold
        assert result["aws_server"] > 0.99    # Well under 100GB threshold

        # 2GB model
        result = compute_size_scores(2 * 1024 * 1024 * 1024)
        assert result["raspberry_pi"] == 0.01  # Over 100MB, hits minimum
        assert result["jetson_nano"] == 0.01   # Over 1GB, hits minimum
        assert result["desktop_pc"] == 0.8     # 2GB of 10GB threshold
        # Well under 100GB threshold (>= to handle precision)
        assert result["aws_server"] >= 0.98


class TestCodeQualityAnalysis:
    """Test code quality analysis functions."""

    def test_analyze_code_quality_empty_files(self):
        """Test code quality analysis with no files."""
        result = analyze_code_quality([])
        expected = {
            "test_coverage_norm": 0.0,
            "style_norm": 0.5,
            "comment_ratio_norm": 0.5,
            "maintainability_norm": 0.5,
        }
        assert result == expected

    def test_analyze_code_quality_with_files(self):
        """Test code quality analysis with typical project files."""
        files = [
            "src/main.py", "src/utils.py", "src/models.py",
            "tests/test_main.py", "tests/test_utils.py",
            "requirements.txt", "README.md", "setup.py"
        ]

        result = analyze_code_quality(files)

        # Should have good test coverage: 2 test files for 3 Python files
        assert result["test_coverage_norm"] > 0.5

        # Should have good maintainability: has requirements, README, setup
        assert result["maintainability_norm"] == 1.0

        # Default values
        assert result["style_norm"] == 0.5
        assert result["comment_ratio_norm"] == 0.5

    def test_analyze_code_quality_no_tests(self):
        """Test code quality analysis with no test files."""
        files = ["src/main.py", "src/utils.py", "requirements.txt"]

        result = analyze_code_quality(files)

        # Should have poor test coverage
        assert result["test_coverage_norm"] == 0.0

        # Should have partial maintainability (has requirements but no README/setup)
        assert result["maintainability_norm"] < 1.0


class TestPerformanceClaimsAnalysis:
    """Test enhanced performance claims analysis."""

    def create_mock_hf_data(self, **kwargs):
        """Create mock HuggingFace model data."""
        default_data = {
            "card_data": {},
            "pipeline_tag": None,
            "downloads": 0
        }
        default_data.update(kwargs)
        return default_data

    def create_mock_card_data(self, **kwargs):
        """Create mock card data that behaves like ModelCardData."""
        mock_card = Mock()
        mock_card.to_dict.return_value = kwargs
        return mock_card

    def test_analyze_performance_claims_minimal(self):
        """Test performance claims analysis with minimal data."""
        hf_data = self.create_mock_hf_data()
        result = analyze_performance_claims(hf_data, [])

        assert result["requirements_passed"] == 0
        assert result["requirements_total"] == 7
        assert result["requirements_score"] == 0.0
        assert not result["details"]["model_index"]
        assert not result["details"]["datasets_mentioned"]

    def test_analyze_performance_claims_with_pipeline_tag(self):
        """Test performance claims analysis with pipeline tag."""
        hf_data = self.create_mock_hf_data(pipeline_tag="text-generation")
        result = analyze_performance_claims(hf_data, [])

        assert result["requirements_passed"] == 1  # Only pipeline_tag
        assert result["requirements_score"] > 0.1  # Should get some score
        assert result["details"]["pipeline_tag"] == "text-generation"

    def test_analyze_performance_claims_with_card_data(self):
        """Test performance claims analysis with rich card data."""
        card_data = self.create_mock_card_data(
            datasets=["squad", "glue"],
            metrics=["accuracy", "f1"],
            **{"model-index": [{"name": "test"}]}
        )
        hf_data = self.create_mock_hf_data(
            card_data=card_data, pipeline_tag="question-answering")

        result = analyze_performance_claims(hf_data, ["README.md"])

        assert result["requirements_passed"] >= 4  # Multiple indicators
        assert result["requirements_score"] > 0.5  # Should get good score
        assert result["details"]["model_index"] is True
        assert result["details"]["datasets_mentioned"] is True
        assert result["details"]["metrics_metadata"] is True
        assert result["details"]["pipeline_tag"] == "question-answering"

    def test_analyze_performance_claims_with_github_files(self):
        """Test performance claims analysis with GitHub files."""
        hf_data = self.create_mock_hf_data()
        github_files = [
            "README.md",
            "evaluate.py",
            "benchmark_results.json",
            "tests/test_model.py"
        ]

        result = analyze_performance_claims(hf_data, github_files)

        assert result["details"]["readme_available"] is True
        # evaluate.py, benchmark_results.json, test_model.py
        assert result["details"]["benchmark_files"] == 3
        # Should get reasonable score (adjusted for actual calculation)
        assert result["requirements_score"] > 0.18

    def test_analyze_performance_claims_card_data_as_dict(self):
        """Test performance claims analysis when card_data is already a dict."""
        hf_data = self.create_mock_hf_data(
            card_data={"datasets": ["test"], "metrics": ["accuracy"]}
        )

        result = analyze_performance_claims(hf_data, [])

        assert result["details"]["datasets_mentioned"] is True
        assert result["details"]["metrics_metadata"] is True


class TestAPIIntegration:
    """Test API integration functions with mocking."""

    @patch('huggingface_hub.model_info')
    @patch('huggingface_hub.HfApi')
    def test_get_huggingface_model_data_success(self, mock_hf_api, mock_model_info):
        """Test successful HuggingFace model data retrieval."""
        # Mock model_info
        mock_info = Mock()
        mock_info.tags = ["pytorch", "bert"]
        mock_info.downloads = 1000000
        mock_info.pipeline_tag = "fill-mask"
        mock_info.modelId = "bert-base-uncased"
        mock_info.sha = "abc123"

        # Mock cardData as a dict-like object that supports .get()
        mock_card_data = {"license": "apache-2.0"}
        mock_info.cardData = mock_card_data
        mock_model_info.return_value = mock_info

        # Mock HfApi
        mock_api = Mock()
        mock_api.list_repo_files.return_value = [
            "config.json", "pytorch_model.bin"]

        # Mock file info - return list with RepoFile-like objects
        mock_file_info = Mock()
        mock_file_info.size = 500000000  # 500MB
        mock_api.get_paths_info.return_value = [mock_file_info]

        mock_hf_api.return_value = mock_api

        result = get_huggingface_model_data(
            "https://huggingface.co/bert-base-uncased")

        assert result["downloads"] == 1000000
        assert result["pipeline_tag"] == "fill-mask"
        assert result["total_size_bytes"] == 1000000000  # 2 files * 500MB each
        assert result["license"] == "apache-2.0"

    @patch('huggingface_hub.model_info')
    def test_get_huggingface_model_data_failure(self, mock_model_info):
        """Test HuggingFace model data retrieval failure."""
        mock_model_info.side_effect = Exception("API error")

        result = get_huggingface_model_data(
            "https://huggingface.co/invalid-model")
        assert result == {}

    @patch('requests.get')
    def test_get_github_repo_data_success(self, mock_get):
        """Test successful GitHub repository data retrieval."""
        # Mock repository response
        repo_data = {
            "stargazers_count": 1000,
            "forks_count": 200,
            "created_at": "2020-01-01T00:00:00Z",
            "updated_at": "2023-01-01T00:00:00Z",
            "license": {"spdx_id": "MIT"}
        }

        # Mock contributors response
        contributors_data = [
            {"contributions": 100},
            {"contributions": 50},
            {"contributions": 25}
        ]

        # Mock tree response
        tree_data = {
            "tree": [
                {"path": "README.md", "type": "blob"},
                {"path": "src/main.py", "type": "blob"},
                {"path": "tests", "type": "tree"}  # Should be ignored
            ]
        }

        def mock_response(*args, **kwargs):
            url = args[0]
            response = Mock()
            response.raise_for_status.return_value = None
            response.ok = True

            if "/repos/" in url and url.endswith("/repo"):
                response.json.return_value = repo_data
            elif "/contributors" in url:
                response.json.return_value = contributors_data
            elif "/git/trees/" in url:
                response.json.return_value = tree_data

            return response

        mock_get.side_effect = mock_response

        result = get_github_repo_data("https://github.com/owner/repo")

        assert result["stars"] == 1000
        assert result["forks"] == 200
        assert result["license"] == "MIT"
        assert len(result["files"]) == 2  # Only blobs, not trees
        assert "README.md" in result["files"]
        assert "src/main.py" in result["files"]

        # Check contributors
        contributors = result["contributors"]
        assert contributors["contributors_count"] == 3
        assert contributors["total_contributions"] == 175
        assert contributors["top_contributor_pct"] == 100 / \
            175  # Top contributor percentage

    @patch('requests.get')
    def test_get_github_repo_data_failure(self, mock_get):
        """Test GitHub repository data retrieval failure."""
        mock_get.side_effect = Exception("API error")

        result = get_github_repo_data("https://github.com/owner/repo")

        # Should return empty structure
        assert result["stars"] == 0
        assert result["forks"] == 0
        assert result["files"] == []
        assert result["contributors"] == {}


class TestComprehensiveFetching:
    """Test the main comprehensive data fetching function."""

    @patch('src.metrics.data_fetcher.get_huggingface_model_data')
    @patch('src.metrics.data_fetcher.get_github_repo_data')
    @patch('src.metrics.data_fetcher.check_availability')
    def test_fetch_comprehensive_metrics_data_success(self, mock_availability, mock_github, mock_hf):
        """Test successful comprehensive data fetching."""
        # Mock availability
        mock_availability.return_value = {
            "has_code": True,
            "has_dataset": True,
            "has_model": True,
            "links_ok": True
        }

        # Mock HuggingFace data
        mock_hf.return_value = {
            "license": "apache-2.0",
            "downloads": 1000000,
            "total_size_bytes": 500000000,
            "pipeline_tag": "text-generation",
            "card_data": Mock()
        }
        mock_hf.return_value["card_data"].to_dict.return_value = {
            "datasets": ["squad"]}

        # Mock GitHub data
        mock_github.return_value = {
            "contributors": {"contributors_count": 5, "top_contributor_pct": 0.4},
            "files": ["README.md", "src/main.py", "tests/test_main.py"],
            "stars": 1000,
            "license": "MIT",
            "updated_at": "2023-01-01T00:00:00Z"
        }

        result = fetch_comprehensive_metrics_data(
            code_url="https://github.com/owner/repo",
            dataset_url="https://huggingface.co/datasets/squad",
            model_url="https://huggingface.co/gpt2"
        )

        # Check all components are present
        assert result["availability"]["links_ok"] is True
        assert result["license"] == "apache-2.0"  # HF license takes precedence
        assert result["repo_meta"]["contributors_count"] == 5
        assert "size_components" in result
        assert result["size_components"]["desktop_pc"] > 0.9  # 500MB model
        # Should have decent performance claims
        assert result["requirements_score"] > 0.3
        assert "ramp" in result
        assert result["ramp"]["downloads_norm"] > 0.9  # 1M downloads

    @patch('src.metrics.data_fetcher.get_huggingface_model_data')
    def test_fetch_comprehensive_metrics_data_hf_failure(self, mock_hf):
        """Test comprehensive data fetching when HuggingFace API fails."""
        mock_hf.return_value = {}  # Empty response simulates failure

        result = fetch_comprehensive_metrics_data(
            code_url="",
            dataset_url="",
            model_url="https://huggingface.co/invalid-model"
        )

        # Should still return valid structure with defaults
        assert result["requirements_passed"] == 0
        assert result["requirements_total"] == 1
        assert result["size_components"] == {
            "raspberry_pi": 0.01, "jetson_nano": 0.01, "desktop_pc": 0.01, "aws_server": 0.01}

    def test_fetch_comprehensive_metrics_data_exception_handling(self):
        """Test comprehensive data fetching with exception handling."""
        # Force an exception by passing invalid data
        with patch('src.metrics.data_fetcher.check_availability', side_effect=Exception("Test error")):
            result = fetch_comprehensive_metrics_data("", "", "")

            # Should return fallback data structure
            assert result["availability"]["links_ok"] is False
            assert result["requirements_passed"] == 0
            assert result["size_components"]["raspberry_pi"] == 0.01


if __name__ == "__main__":
    pytest.main([__file__])
